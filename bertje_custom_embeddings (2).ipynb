{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721e0321-69fa-42b3-a952-ba8a76881aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 12:28:55.491273: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-26 12:28:55.504571: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750940935.519534  998596 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750940935.524102  998596 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750940935.536545  998596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750940935.536566  998596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750940935.536569  998596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750940935.536571  998596 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-26 12:28:55.540633: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaModel, AdamW\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24257b6e-281a-489f-b079-ca47f7e6fae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6039f083-12e3-4a1d-826a-3250d6e5d46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (5894, 25)\n",
      "Test data shape: (722, 25)\n",
      "Training columns: ['Sentence', 'Emotion', 'POS_CCONJ', 'POS_VERB', 'POS_PRON', 'POS_SCONJ', 'POS_DET', 'POS_NOUN', 'POS_PUNCT', 'POS_ADJ', 'POS_ADV', 'POS_ADP', 'POS_PROPN', 'POS_AUX', 'POS_INTJ', 'POS_NUM', 'POS_SPACE', 'POS_SYM', 'POS_X', 'POS_tags', 'TF-IDF', 'Pretrained_Embeddings', 'Custom_Embeddings', 'Sentiment_Score', 'NounChunkCount']\n"
     ]
    }
   ],
   "source": [
    "# Load datasets with custom embeddings\n",
    "training = pd.read_csv(\"data/NLP_features_Train.csv\")\n",
    "test = pd.read_csv(\"data/NLP_features_Test.csv\")\n",
    "\n",
    "print(f\"Training data shape: {training.shape}\")\n",
    "print(f\"Test data shape: {test.shape}\")\n",
    "print(f\"Training columns: {training.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "embedding_processing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing custom embeddings...\n",
      "Custom embeddings shape - Train: (5894, 768), Test: (722, 768)\n"
     ]
    }
   ],
   "source": [
    "# Function to parse custom embeddings (assuming they're stored as strings)\n",
    "def parse_embeddings(embedding_str):\n",
    "    \"\"\"Parse embedding string to numpy array\"\"\"\n",
    "    if pd.isna(embedding_str):\n",
    "        return np.zeros(768)  # Default size, adjust if needed\n",
    "    \n",
    "    try:\n",
    "        # Try to parse as list/array string\n",
    "        if isinstance(embedding_str, str):\n",
    "            # Remove brackets and split by comma\n",
    "            embedding_str = embedding_str.strip('[]')\n",
    "            embedding = np.array([float(x.strip()) for x in embedding_str.split(',')])\n",
    "        else:\n",
    "            embedding = np.array(embedding_str)\n",
    "        return embedding\n",
    "    except:\n",
    "        # If parsing fails, return zeros\n",
    "        return np.zeros(768)\n",
    "\n",
    "# Parse custom embeddings\n",
    "print(\"Parsing custom embeddings...\")\n",
    "train_custom_embeddings = np.array([parse_embeddings(emb) for emb in training['Custom_Embeddings']])\n",
    "test_custom_embeddings = np.array([parse_embeddings(emb) for emb in test['Custom_Embeddings']])\n",
    "\n",
    "print(f\"Custom embeddings shape - Train: {train_custom_embeddings.shape}, Test: {test_custom_embeddings.shape}\")\n",
    "\n",
    "# Normalize custom embeddings\n",
    "scaler = StandardScaler()\n",
    "train_custom_embeddings = scaler.fit_transform(train_custom_embeddings)\n",
    "test_custom_embeddings = scaler.transform(test_custom_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "953f4fe8-684a-4391-8157-4814e7406ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 7\n",
      "Classes: ['anger' 'disgust' 'fear' 'happiness' 'neutral' 'sadness' 'surprise']\n"
     ]
    }
   ],
   "source": [
    "# Define features and target\n",
    "X = training[\"Sentence\"].astype(str)\n",
    "y = training[\"Emotion\"]\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"Number of classes: {len(label_encoder.classes_)}\")\n",
    "print(f\"Classes: {label_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c695332-2549-4c01-a9ec-0557277900f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 2357\n",
      "Validation set size: 589\n",
      "Test set size: 361\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train and validation (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val, train_emb_train, train_emb_val = train_test_split(\n",
    "    X, y_encoded, train_custom_embeddings, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Further reduce dataset size to 30%\n",
    "reduce_fraction = 0.5\n",
    "\n",
    "X_train, _, y_train, _, train_emb_train, _ = train_test_split(\n",
    "    X_train, y_train, train_emb_train, test_size=(1 - reduce_fraction), random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "X_val, _, y_val, _, train_emb_val, _ = train_test_split(\n",
    "    X_val, y_val, train_emb_val, test_size=(1 - reduce_fraction), random_state=42, stratify=y_val\n",
    ")\n",
    "\n",
    "# Load test data\n",
    "X_test = test['Sentence'].astype(str)\n",
    "y_test = test['Emotion']\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Reduce test set size as well\n",
    "X_test, _, y_test_encoded, _ = train_test_split(\n",
    "    X_test, y_test_encoded, test_size=(1 - reduce_fraction), random_state=42, stratify=y_test_encoded\n",
    ")\n",
    "\n",
    "# Final set sizes\n",
    "print(f\"Train set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6befc03-c865-4db2-b2ac-698d144fce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Roberta tokenizer\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bd3db1c-2797-45cb-bfc8-106dfc7c9645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced dataset class with custom embeddings\n",
    "class EnhancedEmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, custom_embeddings, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.custom_embeddings = custom_embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        custom_emb = self.custom_embeddings[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"custom_embedding\": torch.tensor(custom_emb, dtype=torch.float32),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hybrid_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid model combining Roberta and custom embeddings\n",
    "class HybridRobertaModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, custom_embedding_dim, dropout_rate=0.3):\n",
    "        super(HybridRobertaModel, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        roberta_dim = self.roberta.config.hidden_size\n",
    "        \n",
    "        # Custom embedding processing layers\n",
    "        self.custom_projection = nn.Linear(custom_embedding_dim, roberta_dim // 2)\n",
    "        self.custom_activation = nn.ReLU()\n",
    "        self.custom_dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Combined feature dimension\n",
    "        combined_dim = roberta_dim + roberta_dim // 2\n",
    "        \n",
    "        # Classification layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(combined_dim, combined_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(combined_dim // 2, num_labels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, custom_embedding):\n",
    "        # Roberta forward pass\n",
    "        Roberta_outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        Roberta_pooled = Roberta_outputs.pooler_output\n",
    "        Roberta_features = self.dropout(Roberta_pooled)\n",
    "        \n",
    "        # Custom embedding processing\n",
    "        custom_features = self.custom_projection(custom_embedding)\n",
    "        custom_features = self.custom_activation(custom_features)\n",
    "        custom_features = self.custom_dropout(custom_features)\n",
    "        \n",
    "        # Combine features\n",
    "        combined_features = torch.cat([Roberta_features, custom_features], dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(combined_features)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "040822e7-19a6-490e-8053-b741ecb0d93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created datasets and dataloaders successfully\n"
     ]
    }
   ],
   "source": [
    "# Create enhanced datasets\n",
    "train_dataset = EnhancedEmotionDataset(X_train.tolist(), y_train, train_emb_train, tokenizer)\n",
    "val_dataset = EnhancedEmotionDataset(X_val.tolist(), y_val, train_emb_val, tokenizer)\n",
    "test_dataset = EnhancedEmotionDataset(X_test.tolist(), y_test_encoded, test_custom_embeddings, tokenizer)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"Created datasets and dataloaders successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78423487-8606-44c9-a64a-ee862e6d4af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with custom embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "# Initialize hybrid model\n",
    "custom_embedding_dim = train_custom_embeddings.shape[1]\n",
    "model = HybridRobertaModel(\n",
    "    model_name=model_name, \n",
    "    num_labels=len(label_encoder.classes_),\n",
    "    custom_embedding_dim=custom_embedding_dim\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model initialized with custom embedding dimension: {custom_embedding_dim}\")\n",
    "\n",
    "# Training setup\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "\n",
    "def train_epoch(model, train_loader):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    predictions, true_labels = [], []\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        custom_embedding = batch['custom_embedding'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        logits = model(input_ids, attention_mask, custom_embedding)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return total_loss / len(train_loader), accuracy_score(true_labels, predictions)\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            custom_embedding = batch['custom_embedding'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, custom_embedding)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    acc = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    return acc, f1, classification_report(true_labels, predictions), predictions, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "training_loop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "Epoch 1/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8393, Train Acc: 0.2401\n",
      "Val Acc: 0.3158, Val F1: 0.2349\n",
      "Learning Rate: 2.00e-05\n",
      "New best validation accuracy: 0.3158\n",
      "\n",
      "Epoch 2/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6709, Train Acc: 0.3394\n",
      "Val Acc: 0.3735, Val F1: 0.3140\n",
      "Learning Rate: 2.00e-05\n",
      "New best validation accuracy: 0.3735\n",
      "\n",
      "Epoch 3/3\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5225, Train Acc: 0.4018\n",
      "Val Acc: 0.3871, Val F1: 0.3373\n",
      "Learning Rate: 2.00e-05\n",
      "New best validation accuracy: 0.3871\n",
      "\n",
      "Loaded best model with validation accuracy: 0.3871\n"
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "max_patience = 3\n",
    "num_epochs = 3\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Training\n",
    "    train_loss, train_acc = train_epoch(model, train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    val_acc, val_f1, val_report, _, _ = evaluate_model(model, val_loader)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), 'best_hybrid_model.pth')\n",
    "        print(f\"New best validation accuracy: {best_val_acc:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement. Patience: {patience_counter}/{max_patience}\")\n",
    "        \n",
    "        if patience_counter >= max_patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "# Load best model for final evaluation\n",
    "model.load_state_dict(torch.load('best_hybrid_model.pth'))\n",
    "print(f\"\\nLoaded best model with validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "final_evaluation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL TEST EVALUATION\n",
      "============================================================\n",
      "Final Test Accuracy: 0.3075\n",
      "Final Test F1-Score: 0.3113\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        23\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.07      0.78      0.13         9\n",
      "           3       0.51      0.20      0.28       122\n",
      "           4       0.44      0.53      0.48       118\n",
      "           5       0.14      0.20      0.16        15\n",
      "           6       0.29      0.21      0.24        73\n",
      "\n",
      "    accuracy                           0.31       361\n",
      "   macro avg       0.21      0.27      0.19       361\n",
      "weighted avg       0.38      0.31      0.31       361\n",
      "\n",
      "\n",
      "============================================================\n",
      "MODEL PERFORMANCE SUMMARY\n",
      "============================================================\n",
      "Best Validation Accuracy: 0.3871\n",
      "Final Test Accuracy: 0.3075\n",
      "Final Test F1-Score: 0.3113\n",
      "Number of Classes: 7\n",
      "Custom Embedding Dimension: 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation on test set\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL TEST EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_acc, test_f1, test_report, test_predictions, test_true_labels = evaluate_model(model, test_loader)\n",
    "\n",
    "print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Final Test F1-Score: {test_f1:.4f}\")\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(test_report)\n",
    "\n",
    "# Compare with baseline (if you want to show improvement)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Final Test F1-Score: {test_f1:.4f}\")\n",
    "print(f\"Number of Classes: {len(label_encoder.classes_)}\")\n",
    "print(f\"Custom Embedding Dimension: {custom_embedding_dim}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
