{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1b59a06",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae1a86f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from gensim.models import Word2Vec\n",
    "from googletrans import Translator\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5426b226",
   "metadata": {},
   "source": [
    "#### Change your CSV and Spacy language if needed here\n",
    "\n",
    "- Loading the training and test datasets from CSV files.\n",
    "- Importing a custom Dutch simplification corpus from Hugging Face (`wiki_simplifications_dutch_dedup_split`).\n",
    "- Initializing a SpaCy NLP pipeline for Dutch using the `nl_core_news_sm` model. If you want to use the english one: `nlp = spacy.load(\"en_core_web_sm\")`\n",
    "- Displaying the first few rows of the test dataset to inspect its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7555998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Sentence    Emotion\n",
      "0               van jullie het eiland weer verlaten.    neutral\n",
      "1  Maar zie het als een compliment, want eigenlij...  happiness\n",
      "2  zien als de grootste bedreiging voor hun relatie.       fear\n",
      "3                    OkÃ©, hier zijn ze, de koppels!  happiness\n",
      "4  De koppels zien elkaar een laatste keer terug,...    sadness\n"
     ]
    }
   ],
   "source": [
    "# Data used for training\n",
    "training_data = pd.read_csv(\"data/train_dataset.csv\", encoding=\"utf-8\")\n",
    "data = pd.read_csv(\"data/test_dataset.csv\", encoding=\"utf-8\")\n",
    "\n",
    "custom_corpus = load_dataset(\"BramVanroy/wiki_simplifications_dutch_dedup_split\")\n",
    "custom_corpus = custom_corpus[\"train\"].to_pandas()\n",
    "\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d6c502",
   "metadata": {},
   "source": [
    "Drop all the columns we don't use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25be2d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.drop(index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b7e5d1",
   "metadata": {},
   "source": [
    "### Extracting POS Tag Proportions with SpaCy\n",
    "\n",
    "This script calculates the proportion of each Universal POS tag in a text using SpaCy. The steps include:\n",
    "\n",
    "- Defining a list of Universal POS tags as used by SpaCy.\n",
    "- Implementing the `pos_tag_proportions` function, which:\n",
    "  - Processes a text with the active SpaCy NLP pipeline.\n",
    "  - Counts the occurrence of each POS tag (excluding spaces).\n",
    "  - Calculates the relative frequency (proportion) of each POS tag in the text.\n",
    "- Applying this function to the `\"Sentence\"` column of a DataFrame `df`.\n",
    "- Expanding the results into separate columns and merging them back into the original DataFrame.\n",
    "- Displaying the updated DataFrame with added POS proportion features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eab2ab67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Sentence    Emotion  POS_CCONJ  \\\n",
      "1  Maar zie het als een compliment, want eigenlij...  happiness   0.117647   \n",
      "2  zien als de grootste bedreiging voor hun relatie.       fear   0.000000   \n",
      "3                    OkÃ©, hier zijn ze, de koppels!  happiness   0.000000   \n",
      "4  De koppels zien elkaar een laatste keer terug,...    sadness   0.066667   \n",
      "5  Dat is super zenuwachtig, want je weet niet ho...       fear   0.071429   \n",
      "\n",
      "   POS_VERB  POS_PRON  POS_SCONJ   POS_DET  POS_NOUN  POS_PUNCT   POS_ADJ  \\\n",
      "1  0.176471  0.176471   0.117647  0.117647  0.117647   0.058824  0.058824   \n",
      "2  0.111111  0.111111   0.111111  0.111111  0.222222   0.111111  0.111111   \n",
      "3  0.000000  0.100000   0.000000  0.100000  0.100000   0.300000  0.000000   \n",
      "4  0.133333  0.066667   0.000000  0.200000  0.200000   0.133333  0.066667   \n",
      "5  0.071429  0.214286   0.000000  0.000000  0.071429   0.142857  0.000000   \n",
      "\n",
      "    POS_ADV   POS_ADP  POS_PROPN   POS_AUX  POS_INTJ  POS_NUM  POS_SPACE  \\\n",
      "1  0.058824  0.000000        0.0  0.000000       0.0      0.0        0.0   \n",
      "2  0.000000  0.111111        0.0  0.000000       0.0      0.0        0.0   \n",
      "3  0.100000  0.000000        0.2  0.100000       0.0      0.0        0.0   \n",
      "4  0.066667  0.000000        0.0  0.066667       0.0      0.0        0.0   \n",
      "5  0.214286  0.000000        0.0  0.214286       0.0      0.0        0.0   \n",
      "\n",
      "   POS_SYM  POS_X  \n",
      "1      0.0    0.0  \n",
      "2      0.0    0.0  \n",
      "3      0.0    0.0  \n",
      "4      0.0    0.0  \n",
      "5      0.0    0.0  \n"
     ]
    }
   ],
   "source": [
    "# List of all Universal POS tags from spaCy\n",
    "all_pos_tags = [\n",
    "    \"CCONJ\", \"VERB\", \"PRON\", \"SCONJ\", \"DET\", \"NOUN\", \"PUNCT\", \n",
    "    \"ADJ\", \"ADV\", \"ADP\", \"PROPN\", \"AUX\", \"INTJ\", \"NUM\", \"SPACE\", \"SYM\", \"X\"\n",
    "]\n",
    "\n",
    "def pos_tag_proportions(text):\n",
    "    doc = nlp(str(text))\n",
    "    # Extract POS tags (exclude spaces if you want, or include depending on needs)\n",
    "    tags = [token.pos_ for token in doc if not token.is_space]\n",
    "    total = len(tags)\n",
    "    counts = {}\n",
    "    \n",
    "    # Count occurrences for each POS tag\n",
    "    for tag in all_pos_tags:\n",
    "        counts[f\"POS_{tag}\"] = tags.count(tag) / total if total > 0 else 0\n",
    "    \n",
    "    return counts\n",
    "\n",
    "# Apply the function and convert list of dicts to a DataFrame\n",
    "pos_props_df = df[\"Sentence\"].apply(pos_tag_proportions).apply(pd.Series)\n",
    "\n",
    "# Join these columns back to original DataFrame\n",
    "df = pd.concat([df, pos_props_df], axis=1)\n",
    "\n",
    "# Check result\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b3cf55",
   "metadata": {},
   "source": [
    "### Extracting POS Tags from Text with SpaCy\n",
    "\n",
    "This code extracts Universal POS tags from each sentence in a DataFrame using SpaCy. The process includes:\n",
    "\n",
    "- Defining the `extract_pos_tags` function, which:\n",
    "  - Processes a text string with the active SpaCy NLP pipeline.\n",
    "  - Returns a list of POS tags for each token, excluding spaces.\n",
    "- Applying the function to the `\"Sentence\"` column of the DataFrame `df` to create a new column `\"POS_tags\"`, which contains the list of POS tags for each sentence.\n",
    "- Optionally printing the `\"Sentence\"` and corresponding `\"POS_tags\"` columns to inspect the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5093551b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Sentence  \\\n",
      "1  Maar zie het als een compliment, want eigenlij...   \n",
      "2  zien als de grootste bedreiging voor hun relatie.   \n",
      "3                    OkÃ©, hier zijn ze, de koppels!   \n",
      "4  De koppels zien elkaar een laatste keer terug,...   \n",
      "5  Dat is super zenuwachtig, want je weet niet ho...   \n",
      "\n",
      "                                            POS_tags  \n",
      "1  [CCONJ, VERB, PRON, SCONJ, DET, NOUN, PUNCT, C...  \n",
      "2  [VERB, SCONJ, DET, ADJ, NOUN, ADP, PRON, NOUN,...  \n",
      "3  [PROPN, PROPN, PUNCT, ADV, AUX, PRON, PUNCT, D...  \n",
      "4  [DET, NOUN, VERB, PRON, DET, ADJ, NOUN, ADV, P...  \n",
      "5  [PRON, AUX, NOUN, ADV, PUNCT, CCONJ, PRON, VER...  \n"
     ]
    }
   ],
   "source": [
    "def extract_pos_tags(text):\n",
    "    doc = nlp(str(text))\n",
    "    return [token.pos_ for token in doc if not token.is_space]\n",
    "\n",
    "# Apply to dataframe\n",
    "df[\"POS_tags\"] = df[\"Sentence\"].astype(str).apply(extract_pos_tags)\n",
    "\n",
    "# Optional: check the result\n",
    "print(df[[\"Sentence\", \"POS_tags\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea0ea51",
   "metadata": {},
   "source": [
    "### Generating TF-IDF Features\n",
    "\n",
    "Creates TF-IDF features from text data used for training. The steps include:\n",
    "\n",
    "- Initializing a `TfidfVectorizer` from scikit-learn to convert text into numerical feature vectors based on word importance.\n",
    "- Fitting the vectorizer to the `\"Sentence\"` column and transforming the sentences into a TF-IDF matrix.\n",
    "- Converting the TF-IDF matrix to an array and:\n",
    "  - Storing the full TF-IDF vectors as a list in the `\"TF-IDF\"` column.\n",
    "  - Calculating the **mean TF-IDF score** per sentence and storing that value in the `\"TF-IDF\"` column.\n",
    "- Displaying the first few rows of the updated DataFrame and checking the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18f3ecd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Sentence    Emotion  POS_CCONJ  \\\n",
      "1  Maar zie het als een compliment, want eigenlij...  happiness   0.117647   \n",
      "2  zien als de grootste bedreiging voor hun relatie.       fear   0.000000   \n",
      "3                    OkÃ©, hier zijn ze, de koppels!  happiness   0.000000   \n",
      "4  De koppels zien elkaar een laatste keer terug,...    sadness   0.066667   \n",
      "5  Dat is super zenuwachtig, want je weet niet ho...       fear   0.071429   \n",
      "\n",
      "   POS_VERB  POS_PRON  POS_SCONJ   POS_DET  POS_NOUN  POS_PUNCT   POS_ADJ  \\\n",
      "1  0.176471  0.176471   0.117647  0.117647  0.117647   0.058824  0.058824   \n",
      "2  0.111111  0.111111   0.111111  0.111111  0.222222   0.111111  0.111111   \n",
      "3  0.000000  0.100000   0.000000  0.100000  0.100000   0.300000  0.000000   \n",
      "4  0.133333  0.066667   0.000000  0.200000  0.200000   0.133333  0.066667   \n",
      "5  0.071429  0.214286   0.000000  0.000000  0.071429   0.142857  0.000000   \n",
      "\n",
      "   ...   POS_ADP  POS_PROPN   POS_AUX  POS_INTJ  POS_NUM  POS_SPACE  POS_SYM  \\\n",
      "1  ...  0.000000        0.0  0.000000       0.0      0.0        0.0      0.0   \n",
      "2  ...  0.111111        0.0  0.000000       0.0      0.0        0.0      0.0   \n",
      "3  ...  0.000000        0.2  0.100000       0.0      0.0        0.0      0.0   \n",
      "4  ...  0.000000        0.0  0.066667       0.0      0.0        0.0      0.0   \n",
      "5  ...  0.000000        0.0  0.214286       0.0      0.0        0.0      0.0   \n",
      "\n",
      "   POS_X                                           POS_tags    TF-IDF  \n",
      "1    0.0  [CCONJ, VERB, PRON, SCONJ, DET, NOUN, PUNCT, C...  0.003594  \n",
      "2    0.0  [VERB, SCONJ, DET, ADJ, NOUN, ADP, PRON, NOUN,...  0.002600  \n",
      "3    0.0  [PROPN, PROPN, PUNCT, ADV, AUX, PRON, PUNCT, D...  0.002229  \n",
      "4    0.0  [DET, NOUN, VERB, PRON, DET, ADJ, NOUN, ADV, P...  0.003266  \n",
      "5    0.0  [PRON, AUX, NOUN, ADV, PUNCT, CCONJ, PRON, VER...  0.002981  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "Sentence      object\n",
      "Emotion       object\n",
      "POS_CCONJ    float64\n",
      "POS_VERB     float64\n",
      "POS_PRON     float64\n",
      "POS_SCONJ    float64\n",
      "POS_DET      float64\n",
      "POS_NOUN     float64\n",
      "POS_PUNCT    float64\n",
      "POS_ADJ      float64\n",
      "POS_ADV      float64\n",
      "POS_ADP      float64\n",
      "POS_PROPN    float64\n",
      "POS_AUX      float64\n",
      "POS_INTJ     float64\n",
      "POS_NUM      float64\n",
      "POS_SPACE    float64\n",
      "POS_SYM      float64\n",
      "POS_X        float64\n",
      "POS_tags      object\n",
      "TF-IDF       float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the sentences\n",
    "tfidf_matrix = vectorizer.fit_transform(df[\"Sentence\"].astype(str))\n",
    "\n",
    "\n",
    "df[\"TF-IDF\"] = list(tfidf_matrix.toarray())\n",
    "df[\"TF-IDF\"] = np.mean(tfidf_matrix.toarray(), axis=1)\n",
    "\n",
    "# Show first few rows\n",
    "print(df.head())\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9ec1e6",
   "metadata": {},
   "source": [
    "### Pretrained Word Embedding Generation\n",
    "\n",
    "Prepares text data by tokenizing, lemmatizing, and generating sentence-level embedding features using a pretrained Word2Vec model:\n",
    "\n",
    "#### Loading Pretrained Word Embeddings\n",
    "- A pretrained Word2Vec model (`GoogleNews-vectors-negative300.bin`) is loaded using Gensim's `KeyedVectors`.\n",
    "- The dimensionality of the word embeddings is retrieved with `embedding_dim`.\n",
    "\n",
    "#### Computing Sentence-Level Average Embeddings\n",
    "- The `get_average_embedding` function:\n",
    "  - Iterates over each token in a token list.\n",
    "  - Checks if the token exists in the pretrained model's vocabulary.\n",
    "  - Collects valid word vectors and computes their mean to obtain a single embedding per sentence.\n",
    "  - If no valid tokens are found, returns a zero vector of appropriate dimension.\n",
    "- The sentence-level average embeddings are computed for all token lists and stored in the `\"Pretrained_Embeddings\"` column of `df`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e6ed945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    [maar, zien, het, als, een, compliment, ,, wan...\n",
      "2    [zien, als, de, groot, bedreiging, voor, hun, ...\n",
      "3        [okã, ©, ,, hier, zijn, ze, ,, de, koppel, !]\n",
      "4    [de, koppel, zien, elkaar, een, laat, keer, te...\n",
      "5    [dat, zijn, super, zenuwachtig, ,, want, je, w...\n",
      "Name: Sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['Sentence'] = df[\"Sentence\"].astype(str)\n",
    "\n",
    "def spacy_tokenizer(text):\n",
    "    doc = nlp(text) \n",
    "    tokens = [token.lemma_.lower() for token in doc]\n",
    "    return tokens\n",
    "\n",
    "tokens = df['Sentence'].apply(spacy_tokenizer)\n",
    "print(tokens.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7904592",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_path = 'data/GoogleNews-vectors-negative300.bin'\n",
    "pretrained_model = KeyedVectors.load_word2vec_format(pretrained_path, binary=True)\n",
    "embedding_dim = pretrained_model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f72ea277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_embedding(token_list, model, embedding_dim):\n",
    "    valid_vectors = []\n",
    "    for token in token_list:\n",
    "        if token in model.key_to_index:\n",
    "            valid_vectors.append(model[token])\n",
    "    if not valid_vectors:\n",
    "        return np.zeros(embedding_dim)\n",
    "    return np.mean(valid_vectors, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "avg_embeddings = []\n",
    "for token_list in tokens:\n",
    "    vec = get_average_embedding(token_list, pretrained_model, embedding_dim)\n",
    "    avg_embeddings.append(vec)\n",
    "\n",
    "df['Pretrained_Embeddings'] = avg_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0902d65e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>POS_CCONJ</th>\n",
       "      <th>POS_VERB</th>\n",
       "      <th>POS_PRON</th>\n",
       "      <th>POS_SCONJ</th>\n",
       "      <th>POS_DET</th>\n",
       "      <th>POS_NOUN</th>\n",
       "      <th>POS_PUNCT</th>\n",
       "      <th>POS_ADJ</th>\n",
       "      <th>...</th>\n",
       "      <th>POS_PROPN</th>\n",
       "      <th>POS_AUX</th>\n",
       "      <th>POS_INTJ</th>\n",
       "      <th>POS_NUM</th>\n",
       "      <th>POS_SPACE</th>\n",
       "      <th>POS_SYM</th>\n",
       "      <th>POS_X</th>\n",
       "      <th>POS_tags</th>\n",
       "      <th>TF-IDF</th>\n",
       "      <th>Pretrained_Embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maar zie het als een compliment, want eigenlij...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[CCONJ, VERB, PRON, SCONJ, DET, NOUN, PUNCT, C...</td>\n",
       "      <td>0.003594</td>\n",
       "      <td>[0.012676239, 0.17316228, 0.0743214, 0.0382080...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zien als de grootste bedreiging voor hun relatie.</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[VERB, SCONJ, DET, ADJ, NOUN, ADP, PRON, NOUN,...</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>[-0.029785156, 0.12813313, 0.04682414, 0.03367...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OkÃ©, hier zijn ze, de koppels!</td>\n",
       "      <td>happiness</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[PROPN, PROPN, PUNCT, ADV, AUX, PRON, PUNCT, D...</td>\n",
       "      <td>0.002229</td>\n",
       "      <td>[0.020166015, 0.1265625, 0.095947266, -0.08520...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>De koppels zien elkaar een laatste keer terug,...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[DET, NOUN, VERB, PRON, DET, ADJ, NOUN, ADV, P...</td>\n",
       "      <td>0.003266</td>\n",
       "      <td>[0.016427612, 0.07313013, 0.015861511, -0.0024...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dat is super zenuwachtig, want je weet niet ho...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[PRON, AUX, NOUN, ADV, PUNCT, CCONJ, PRON, VER...</td>\n",
       "      <td>0.002981</td>\n",
       "      <td>[0.10219505, 0.19249378, 0.0288835, 0.03053977...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence    Emotion  POS_CCONJ  \\\n",
       "1  Maar zie het als een compliment, want eigenlij...  happiness   0.117647   \n",
       "2  zien als de grootste bedreiging voor hun relatie.       fear   0.000000   \n",
       "3                    OkÃ©, hier zijn ze, de koppels!  happiness   0.000000   \n",
       "4  De koppels zien elkaar een laatste keer terug,...    sadness   0.066667   \n",
       "5  Dat is super zenuwachtig, want je weet niet ho...       fear   0.071429   \n",
       "\n",
       "   POS_VERB  POS_PRON  POS_SCONJ   POS_DET  POS_NOUN  POS_PUNCT   POS_ADJ  \\\n",
       "1  0.176471  0.176471   0.117647  0.117647  0.117647   0.058824  0.058824   \n",
       "2  0.111111  0.111111   0.111111  0.111111  0.222222   0.111111  0.111111   \n",
       "3  0.000000  0.100000   0.000000  0.100000  0.100000   0.300000  0.000000   \n",
       "4  0.133333  0.066667   0.000000  0.200000  0.200000   0.133333  0.066667   \n",
       "5  0.071429  0.214286   0.000000  0.000000  0.071429   0.142857  0.000000   \n",
       "\n",
       "   ...  POS_PROPN   POS_AUX  POS_INTJ  POS_NUM  POS_SPACE  POS_SYM  POS_X  \\\n",
       "1  ...        0.0  0.000000       0.0      0.0        0.0      0.0    0.0   \n",
       "2  ...        0.0  0.000000       0.0      0.0        0.0      0.0    0.0   \n",
       "3  ...        0.2  0.100000       0.0      0.0        0.0      0.0    0.0   \n",
       "4  ...        0.0  0.066667       0.0      0.0        0.0      0.0    0.0   \n",
       "5  ...        0.0  0.214286       0.0      0.0        0.0      0.0    0.0   \n",
       "\n",
       "                                            POS_tags    TF-IDF  \\\n",
       "1  [CCONJ, VERB, PRON, SCONJ, DET, NOUN, PUNCT, C...  0.003594   \n",
       "2  [VERB, SCONJ, DET, ADJ, NOUN, ADP, PRON, NOUN,...  0.002600   \n",
       "3  [PROPN, PROPN, PUNCT, ADV, AUX, PRON, PUNCT, D...  0.002229   \n",
       "4  [DET, NOUN, VERB, PRON, DET, ADJ, NOUN, ADV, P...  0.003266   \n",
       "5  [PRON, AUX, NOUN, ADV, PUNCT, CCONJ, PRON, VER...  0.002981   \n",
       "\n",
       "                               Pretrained_Embeddings  \n",
       "1  [0.012676239, 0.17316228, 0.0743214, 0.0382080...  \n",
       "2  [-0.029785156, 0.12813313, 0.04682414, 0.03367...  \n",
       "3  [0.020166015, 0.1265625, 0.095947266, -0.08520...  \n",
       "4  [0.016427612, 0.07313013, 0.015861511, -0.0024...  \n",
       "5  [0.10219505, 0.19249378, 0.0288835, 0.03053977...  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a35bdc",
   "metadata": {},
   "source": [
    "### Training Custom Word2Vec Embeddings and Generating Sentence-Level Features\n",
    "\n",
    "This code trains a custom Word2Vec embedding model on a subset of the `custom_corpus` dataset and computes sentence-level average embeddings for the main DataFrame `df`. The process includes:\n",
    "\n",
    "#### Preprocessing Custom Corpus\n",
    "- The `'prompt'` column of `custom_corpus` is converted to string type.\n",
    "- Only the first 10,000 rows are selected for efficiency.\n",
    "- The `spacy_tokenizer` function (lemmatization and lowercasing) is applied to tokenize the prompts, generating token lists for training.\n",
    "\n",
    "#### Training a Custom Word2Vec Model\n",
    "- A Word2Vec model is trained on the tokenized custom corpus using the Gensim library with parameters:\n",
    "  - Embedding dimension: 100\n",
    "  - Context window size: 5\n",
    "  - Minimum token count threshold: 10 (to ignore infrequent words)\n",
    "  - Parallel workers: 40 (for faster training)\n",
    "  - Number of epochs: 5\n",
    "- The resulting word vectors are stored in `custom_word_vectors`.\n",
    "\n",
    "#### Computing Average Custom Embeddings\n",
    "- The function `get_average_custom_embedding` calculates the mean embedding vector for a list of tokens using the custom-trained embeddings.\n",
    "- For each sentence in the main DataFrame (`df`), the average embedding is computed based on its tokens and stored in the `\"Custom_Embeddings\"` column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570c9d8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Apply tokenizer to the reduced dataset\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m custom_corpus_small_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_corpus_small\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspacy_tokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Show first few token lists\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(custom_corpus_small\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\blockC-y2\\lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\blockC-y2\\lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\blockC-y2\\lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\blockC-y2\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\blockC-y2\\lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[26], line 9\u001b[0m, in \u001b[0;36mspacy_tokenizer\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspacy_tokenizer\u001b[39m(text):\n\u001b[1;32m----> 9\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mlemma_\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc]\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\blockC-y2\\lib\\site-packages\\spacy\\language.py:1052\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1050\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1052\u001b[0m     doc \u001b[38;5;241m=\u001b[39m proc(doc, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcomponent_cfg\u001b[38;5;241m.\u001b[39mget(name, {}))  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\blockC-y2\\lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\blockC-y2\\lib\\site-packages\\spacy\\pipeline\\tok2vec.py:126\u001b[0m, in \u001b[0;36mTok2Vec.predict\u001b[1;34m(self, docs)\u001b[0m\n\u001b[0;32m    124\u001b[0m     width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnO\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39malloc((\u001b[38;5;241m0\u001b[39m, width)) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[1;32m--> 126\u001b[0m tokvecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokvecs\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\blockC-y2\\lib\\site-packages\\thinc\\model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[0;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\blockC-y2\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\blockC-y2\\lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\blockC-y2\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\blockC-y2\\lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\blockC-y2\\lib\\site-packages\\thinc\\layers\\with_array.py:36\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, Xseq, is_train)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m     33\u001b[0m     model: Model[SeqT, SeqT], Xseq: SeqT, is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[0;32m     34\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[SeqT, Callable]:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(Xseq, Ragged):\n\u001b[1;32m---> 36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], \u001b[43m_ragged_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(Xseq, Padded):\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], _padded_forward(model, Xseq, is_train))\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\blockC-y2\\lib\\site-packages\\thinc\\layers\\with_array.py:91\u001b[0m, in \u001b[0;36m_ragged_forward\u001b[1;34m(model, Xr, is_train)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ragged_forward\u001b[39m(\n\u001b[0;32m     88\u001b[0m     model: Model[SeqT, SeqT], Xr: Ragged, is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[0;32m     89\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Ragged, Callable]:\n\u001b[0;32m     90\u001b[0m     layer: Model[ArrayXd, ArrayXd] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 91\u001b[0m     Y, get_dX \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataXd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(dYr: Ragged) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Ragged:\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Ragged(get_dX(dYr\u001b[38;5;241m.\u001b[39mdataXd), dYr\u001b[38;5;241m.\u001b[39mlengths)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\blockC-y2\\lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\blockC-y2\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\blockC-y2\\lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\blockC-y2\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\blockC-y2\\lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\anaconda3\\envs\\blockC-y2\\lib\\site-packages\\thinc\\layers\\maxout.py:52\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     50\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape2f(W, nO \u001b[38;5;241m*\u001b[39m nP, nI)\n\u001b[1;32m---> 52\u001b[0m Y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m Y \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape1f(b, nO \u001b[38;5;241m*\u001b[39m nP)\n\u001b[0;32m     54\u001b[0m Z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape3f(Y, Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], nO, nP)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ensure 'prompt' is string type\n",
    "custom_corpus['prompt'] = custom_corpus['prompt'].astype(str)\n",
    "\n",
    "# Limit to the first 10,000 rows for efficiency\n",
    "custom_corpus_small = custom_corpus.head(10_000)\n",
    "\n",
    "# Define tokenizer with lemmatization and lowercasing\n",
    "def spacy_tokenizer(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower() for token in doc]\n",
    "    return tokens\n",
    "\n",
    "# Apply tokenizer to the reduced dataset\n",
    "custom_corpus_small_tokens = custom_corpus_small['prompt'].apply(spacy_tokenizer)\n",
    "\n",
    "# Show first few token lists\n",
    "print(custom_corpus_small_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620b0a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_embedding_dim = 100\n",
    "custom_model = Word2Vec(\n",
    "    sentences=custom_corpus_small_tokens, \n",
    "    vector_size=custom_embedding_dim,\n",
    "    window=5,          \n",
    "    min_count=10,       \n",
    "    workers=40,         \n",
    "    epochs=5           \n",
    ")\n",
    "\n",
    "custom_word_vectors = custom_model.wv\n",
    "embedding_dim_custom = custom_model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3f09d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_custom_embedding(token_list, model, embedding_dim):\n",
    "    valid_vectors = []\n",
    "    for token in token_list:\n",
    "        if token in model.key_to_index:\n",
    "            valid_vectors.append(model[token])\n",
    "    if not valid_vectors:\n",
    "        return np.zeros(embedding_dim)\n",
    "    return np.mean(valid_vectors, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "custom_avg_embeddings = []\n",
    "for token_list in tokens:\n",
    "    vec = get_average_custom_embedding(token_list, custom_word_vectors, embedding_dim_custom)\n",
    "    custom_avg_embeddings.append(vec)\n",
    "\n",
    "df['Custom_Embeddings'] = custom_avg_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ba8802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>POS_CCONJ</th>\n",
       "      <th>POS_VERB</th>\n",
       "      <th>POS_PRON</th>\n",
       "      <th>POS_SCONJ</th>\n",
       "      <th>POS_DET</th>\n",
       "      <th>POS_NOUN</th>\n",
       "      <th>POS_PUNCT</th>\n",
       "      <th>POS_ADJ</th>\n",
       "      <th>...</th>\n",
       "      <th>POS_AUX</th>\n",
       "      <th>POS_INTJ</th>\n",
       "      <th>POS_NUM</th>\n",
       "      <th>POS_SPACE</th>\n",
       "      <th>POS_SYM</th>\n",
       "      <th>POS_X</th>\n",
       "      <th>POS_tags</th>\n",
       "      <th>TF-IDF</th>\n",
       "      <th>Pretrained_Embeddings</th>\n",
       "      <th>Custom_Embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bewust, niet bewust. ik vind het goed DAT het is.</td>\n",
       "      <td>surprise</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[ADJ, PUNCT, ADV, ADJ, PUNCT, PRON, VERB, PRON...</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>[0.059051514, 0.21899414, 0.003074646, -0.0087...</td>\n",
       "      <td>[-0.021206133, 0.14562613, -0.037825022, -0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Er fysiek wel oordelen open, maar het is niet.</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[ADV, ADJ, ADV, VERB, ADJ, PUNCT, CCONJ, PRON,...</td>\n",
       "      <td>0.000829</td>\n",
       "      <td>[0.04988752, 0.14551653, -0.0022495815, 0.0167...</td>\n",
       "      <td>[-0.01877182, 0.13091229, -0.03340487, -0.0473...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Er staat fontein oordelen open, maar het is niet.</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[ADV, VERB, ADJ, VERB, ADJ, PUNCT, CCONJ, PRON...</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>[0.0663147, 0.14313616, -0.0046561104, 0.01651...</td>\n",
       "      <td>[-0.018213319, 0.13521256, -0.03366551, -0.048...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Er staat bron voor open, maar het are niet.</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[ADV, VERB, NOUN, ADP, ADJ, PUNCT, CCONJ, DET,...</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>[0.009531657, 0.08696832, 0.008748372, 0.01874...</td>\n",
       "      <td>[-0.016320882, 0.12591653, -0.03068872, -0.045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kram doen.</td>\n",
       "      <td>disgust</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[NOUN, VERB, PUNCT]</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>[-0.04345703, -0.0020446777, 0.13183594, 0.025...</td>\n",
       "      <td>[-0.024421837, 0.16432925, -0.043763738, -0.06...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence   Emotion  POS_CCONJ  \\\n",
       "1  Bewust, niet bewust. ik vind het goed DAT het is.  surprise   0.000000   \n",
       "2     Er fysiek wel oordelen open, maar het is niet.   sadness   0.090909   \n",
       "3  Er staat fontein oordelen open, maar het is niet.   sadness   0.090909   \n",
       "4        Er staat bron voor open, maar het are niet.   sadness   0.090909   \n",
       "5                                         kram doen.   disgust   0.000000   \n",
       "\n",
       "   POS_VERB  POS_PRON  POS_SCONJ   POS_DET  POS_NOUN  POS_PUNCT   POS_ADJ  \\\n",
       "1  0.076923  0.230769   0.076923  0.000000  0.000000   0.230769  0.230769   \n",
       "2  0.090909  0.090909   0.000000  0.000000  0.000000   0.181818  0.181818   \n",
       "3  0.181818  0.090909   0.000000  0.000000  0.000000   0.181818  0.181818   \n",
       "4  0.090909  0.000000   0.000000  0.090909  0.090909   0.181818  0.181818   \n",
       "5  0.333333  0.000000   0.000000  0.000000  0.333333   0.333333  0.000000   \n",
       "\n",
       "   ...   POS_AUX  POS_INTJ  POS_NUM  POS_SPACE  POS_SYM  POS_X  \\\n",
       "1  ...  0.076923       0.0      0.0        0.0      0.0    0.0   \n",
       "2  ...  0.090909       0.0      0.0        0.0      0.0    0.0   \n",
       "3  ...  0.090909       0.0      0.0        0.0      0.0    0.0   \n",
       "4  ...  0.000000       0.0      0.0        0.0      0.0    0.0   \n",
       "5  ...  0.000000       0.0      0.0        0.0      0.0    0.0   \n",
       "\n",
       "                                            POS_tags    TF-IDF  \\\n",
       "1  [ADJ, PUNCT, ADV, ADJ, PUNCT, PRON, VERB, PRON...  0.000674   \n",
       "2  [ADV, ADJ, ADV, VERB, ADJ, PUNCT, CCONJ, PRON,...  0.000829   \n",
       "3  [ADV, VERB, ADJ, VERB, ADJ, PUNCT, CCONJ, PRON...  0.000845   \n",
       "4  [ADV, VERB, NOUN, ADP, ADJ, PUNCT, CCONJ, DET,...  0.000854   \n",
       "5                                [NOUN, VERB, PUNCT]  0.000423   \n",
       "\n",
       "                               Pretrained_Embeddings  \\\n",
       "1  [0.059051514, 0.21899414, 0.003074646, -0.0087...   \n",
       "2  [0.04988752, 0.14551653, -0.0022495815, 0.0167...   \n",
       "3  [0.0663147, 0.14313616, -0.0046561104, 0.01651...   \n",
       "4  [0.009531657, 0.08696832, 0.008748372, 0.01874...   \n",
       "5  [-0.04345703, -0.0020446777, 0.13183594, 0.025...   \n",
       "\n",
       "                                   Custom_Embeddings  \n",
       "1  [-0.021206133, 0.14562613, -0.037825022, -0.05...  \n",
       "2  [-0.01877182, 0.13091229, -0.03340487, -0.0473...  \n",
       "3  [-0.018213319, 0.13521256, -0.03366551, -0.048...  \n",
       "4  [-0.016320882, 0.12591653, -0.03068872, -0.045...  \n",
       "5  [-0.024421837, 0.16432925, -0.043763738, -0.06...  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d6e24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Sentence', 'Emotion', 'POS_CCONJ', 'POS_VERB', 'POS_PRON', 'POS_SCONJ',\n",
      "       'POS_DET', 'POS_NOUN', 'POS_PUNCT', 'POS_ADJ', 'POS_ADV', 'POS_ADP',\n",
      "       'POS_PROPN', 'POS_AUX', 'POS_INTJ', 'POS_NUM', 'POS_SPACE', 'POS_SYM',\n",
      "       'POS_X', 'POS_tags', 'TF-IDF', 'Pretrained_Embeddings',\n",
      "       'Custom_Embeddings'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee91c97",
   "metadata": {},
   "source": [
    "### Pretrained Word Embeddings in Machine Learning\n",
    "\n",
    "Implements a basic text classification pipeline that predicts emotions based on average word embeddings extracted from sentences:\n",
    "\n",
    "#### Computing Average Word Embeddings\n",
    "- The function `get_avg_embedding`:\n",
    "  - Splits each sentence into tokens (words).\n",
    "  - Retrieves the pretrained embedding vector for each token if available.\n",
    "  - Computes the mean embedding vector for the sentence.\n",
    "  - Returns a zero vector if no tokens are found in the embedding model.\n",
    "- Average embeddings are computed for each sentence in the `data` DataFrame and stored in a new `\"avg_embed\"` column.\n",
    "\n",
    "#### Preparing Features and Labels\n",
    "- The feature matrix `X` is constructed by stacking all average embedding vectors.\n",
    "- The target labels `y` are extracted from the `\"Emotion\"` column.\n",
    "\n",
    "#### Training and Evaluating a Logistic Regression Model\n",
    "- The dataset is split into training (80%) and testing (20%) subsets.\n",
    "- A `LogisticRegression` classifier is trained on the training embeddings and labels.\n",
    "- Predictions are made on the test set.\n",
    "- The classification performance is evaluated and summarized with `classification_report`, showing precision, recall, F1-score, and support for each emotion class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac7d89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.68      0.24      0.36        54\n",
      "     disgust       0.47      0.23      0.31        69\n",
      "        fear       0.45      0.51      0.48       195\n",
      "   happiness       0.37      0.24      0.29       182\n",
      "     neutral       0.35      0.56      0.43       231\n",
      "     sadness       0.39      0.37      0.38       208\n",
      "    surprise       0.49      0.46      0.47       240\n",
      "\n",
      "    accuracy                           0.41      1179\n",
      "   macro avg       0.46      0.37      0.39      1179\n",
      "weighted avg       0.43      0.41      0.41      1179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def get_avg_embedding(sentence, model, dim):\n",
    "    tokens = sentence.split()\n",
    "    vectors = [model[word] for word in tokens if word in model]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(dim)\n",
    "\n",
    "# Compute embeddings on the test set (data)\n",
    "data[\"avg_embed\"] = data[\"Sentence\"].astype(str).apply(\n",
    "    lambda s: get_avg_embedding(s, pretrained_model, embedding_dim)\n",
    ")\n",
    "\n",
    "# Feature matrix\n",
    "X = np.vstack(data[\"avg_embed\"].values)\n",
    "\n",
    "# Labels\n",
    "y = data[\"Emotion\"].astype(str)  # or replace with correct label column name\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174d110ec614abf6",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "Performs the following steps to analyze sentiment for non-English sentences:\n",
    "\n",
    "#### Translation to English\n",
    "- Uses the `googletrans` library's `Translator` with a lightweight service URL (`translate.googleapis.com`).\n",
    "- Defines an asynchronous function `translate_bulk` to translate a list of sentences (`df['Sentence']`) to English in bulk.\n",
    "- Collects all translated texts in `translations_list`.\n",
    "\n",
    "#### Sentiment Analysis on Translated Text\n",
    "- Initializes the `VADER SentimentIntensityAnalyzer`, a lexicon and rule-based sentiment analysis tool optimized for English.\n",
    "- Applies VADER’s `polarity_scores` method to each translated sentence, generating sentiment scores including:\n",
    "  - Positive, negative, neutral, and compound scores.\n",
    "- Extracts the `compound` score, which summarizes overall sentiment polarity, and stores it in the original DataFrame under the `\"Sentiment_Score\"` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdb26af9fea4aea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T15:13:07.289117Z",
     "start_time": "2025-02-17T15:12:52.353506Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>POS_CCONJ</th>\n",
       "      <th>POS_VERB</th>\n",
       "      <th>POS_PRON</th>\n",
       "      <th>POS_SCONJ</th>\n",
       "      <th>POS_DET</th>\n",
       "      <th>POS_NOUN</th>\n",
       "      <th>POS_PUNCT</th>\n",
       "      <th>POS_ADJ</th>\n",
       "      <th>...</th>\n",
       "      <th>POS_INTJ</th>\n",
       "      <th>POS_NUM</th>\n",
       "      <th>POS_SPACE</th>\n",
       "      <th>POS_SYM</th>\n",
       "      <th>POS_X</th>\n",
       "      <th>POS_tags</th>\n",
       "      <th>TF-IDF</th>\n",
       "      <th>Pretrained_Embeddings</th>\n",
       "      <th>Custom_Embeddings</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bewust, niet bewust. ik vind het goed DAT het is.</td>\n",
       "      <td>surprise</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[ADJ, PUNCT, ADV, ADJ, PUNCT, PRON, VERB, PRON...</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>[0.059051514, 0.21899414, 0.003074646, -0.0087...</td>\n",
       "      <td>[-0.021206133, 0.14562613, -0.037825022, -0.05...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Er fysiek wel oordelen open, maar het is niet.</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[ADV, ADJ, ADV, VERB, ADJ, PUNCT, CCONJ, PRON,...</td>\n",
       "      <td>0.000829</td>\n",
       "      <td>[0.04988752, 0.14551653, -0.0022495815, 0.0167...</td>\n",
       "      <td>[-0.01877182, 0.13091229, -0.03340487, -0.0473...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Er staat fontein oordelen open, maar het is niet.</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[ADV, VERB, ADJ, VERB, ADJ, PUNCT, CCONJ, PRON...</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>[0.0663147, 0.14313616, -0.0046561104, 0.01651...</td>\n",
       "      <td>[-0.018213319, 0.13521256, -0.03366551, -0.048...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Er staat bron voor open, maar het are niet.</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[ADV, VERB, NOUN, ADP, ADJ, PUNCT, CCONJ, DET,...</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>[0.009531657, 0.08696832, 0.008748372, 0.01874...</td>\n",
       "      <td>[-0.016320882, 0.12591653, -0.03068872, -0.045...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kram doen.</td>\n",
       "      <td>disgust</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[NOUN, VERB, PUNCT]</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>[-0.04345703, -0.0020446777, 0.13183594, 0.025...</td>\n",
       "      <td>[-0.024421837, 0.16432925, -0.043763738, -0.06...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5890</th>\n",
       "      <td>En klauteren er dan op.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[CCONJ, VERB, ADV, ADV, ADP, PUNCT]</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>[-0.0087890625, 0.16479492, -0.041625977, 0.01...</td>\n",
       "      <td>[-0.012502238, 0.1236767, -0.031620137, -0.048...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5891</th>\n",
       "      <td>Als je er eenmaal op staat dan manoeuvreer je ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[SCONJ, PRON, ADV, ADV, ADP, VERB, ADV, VERB, ...</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>[0.07126465, 0.19904785, -0.03173828, 0.031010...</td>\n",
       "      <td>[-0.01296855, 0.11175762, -0.027165066, -0.043...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5892</th>\n",
       "      <td>Je teamgenoten die hier aan de touw staan...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[PRON, NOUN, PRON, ADV, ADP, DET, NOUN, VERB, ...</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>[0.034505207, 0.1398112, 0.07434082, 0.0005086...</td>\n",
       "      <td>[-0.018213194, 0.09685853, -0.028583333, -0.03...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5893</th>\n",
       "      <td>bepalen het tempo.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[VERB, DET, NOUN, PUNCT]</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>[0.21679688, 0.3828125, 0.024902344, -0.039306...</td>\n",
       "      <td>[-0.020713434, 0.16700783, -0.042052798, -0.05...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5894</th>\n",
       "      <td>Als je helemaal dichtbij genoeg bent...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[SCONJ, PRON, ADV, ADJ, ADV, AUX, PUNCT]</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>[0.10986328, 0.2298177, 0.0029296875, 0.040974...</td>\n",
       "      <td>[-0.021759177, 0.10952186, -0.02987954, -0.040...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5894 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentence   Emotion  POS_CCONJ  \\\n",
       "1     Bewust, niet bewust. ik vind het goed DAT het is.  surprise   0.000000   \n",
       "2        Er fysiek wel oordelen open, maar het is niet.   sadness   0.090909   \n",
       "3     Er staat fontein oordelen open, maar het is niet.   sadness   0.090909   \n",
       "4           Er staat bron voor open, maar het are niet.   sadness   0.090909   \n",
       "5                                            kram doen.   disgust   0.000000   \n",
       "...                                                 ...       ...        ...   \n",
       "5890                            En klauteren er dan op.   neutral   0.166667   \n",
       "5891  Als je er eenmaal op staat dan manoeuvreer je ...   neutral   0.000000   \n",
       "5892       Je teamgenoten die hier aan de touw staan...   neutral   0.000000   \n",
       "5893                                 bepalen het tempo.   neutral   0.000000   \n",
       "5894            Als je helemaal dichtbij genoeg bent...   neutral   0.000000   \n",
       "\n",
       "      POS_VERB  POS_PRON  POS_SCONJ   POS_DET  POS_NOUN  POS_PUNCT   POS_ADJ  \\\n",
       "1     0.076923  0.230769   0.076923  0.000000  0.000000   0.230769  0.230769   \n",
       "2     0.090909  0.090909   0.000000  0.000000  0.000000   0.181818  0.181818   \n",
       "3     0.181818  0.090909   0.000000  0.000000  0.000000   0.181818  0.181818   \n",
       "4     0.090909  0.000000   0.000000  0.090909  0.090909   0.181818  0.181818   \n",
       "5     0.333333  0.000000   0.000000  0.000000  0.333333   0.333333  0.000000   \n",
       "...        ...       ...        ...       ...       ...        ...       ...   \n",
       "5890  0.166667  0.000000   0.000000  0.000000  0.000000   0.166667  0.000000   \n",
       "5891  0.153846  0.153846   0.076923  0.076923  0.076923   0.076923  0.000000   \n",
       "5892  0.111111  0.222222   0.000000  0.111111  0.222222   0.111111  0.000000   \n",
       "5893  0.250000  0.000000   0.000000  0.250000  0.250000   0.250000  0.000000   \n",
       "5894  0.000000  0.142857   0.142857  0.000000  0.000000   0.142857  0.142857   \n",
       "\n",
       "      ...  POS_INTJ  POS_NUM  POS_SPACE  POS_SYM  POS_X  \\\n",
       "1     ...       0.0      0.0        0.0      0.0    0.0   \n",
       "2     ...       0.0      0.0        0.0      0.0    0.0   \n",
       "3     ...       0.0      0.0        0.0      0.0    0.0   \n",
       "4     ...       0.0      0.0        0.0      0.0    0.0   \n",
       "5     ...       0.0      0.0        0.0      0.0    0.0   \n",
       "...   ...       ...      ...        ...      ...    ...   \n",
       "5890  ...       0.0      0.0        0.0      0.0    0.0   \n",
       "5891  ...       0.0      0.0        0.0      0.0    0.0   \n",
       "5892  ...       0.0      0.0        0.0      0.0    0.0   \n",
       "5893  ...       0.0      0.0        0.0      0.0    0.0   \n",
       "5894  ...       0.0      0.0        0.0      0.0    0.0   \n",
       "\n",
       "                                               POS_tags    TF-IDF  \\\n",
       "1     [ADJ, PUNCT, ADV, ADJ, PUNCT, PRON, VERB, PRON...  0.000674   \n",
       "2     [ADV, ADJ, ADV, VERB, ADJ, PUNCT, CCONJ, PRON,...  0.000829   \n",
       "3     [ADV, VERB, ADJ, VERB, ADJ, PUNCT, CCONJ, PRON...  0.000845   \n",
       "4     [ADV, VERB, NOUN, ADP, ADJ, PUNCT, CCONJ, DET,...  0.000854   \n",
       "5                                   [NOUN, VERB, PUNCT]  0.000423   \n",
       "...                                                 ...       ...   \n",
       "5890                [CCONJ, VERB, ADV, ADV, ADP, PUNCT]  0.000605   \n",
       "5891  [SCONJ, PRON, ADV, ADV, ADP, VERB, ADV, VERB, ...  0.000905   \n",
       "5892  [PRON, NOUN, PRON, ADV, ADP, DET, NOUN, VERB, ...  0.000786   \n",
       "5893                           [VERB, DET, NOUN, PUNCT]  0.000476   \n",
       "5894           [SCONJ, PRON, ADV, ADJ, ADV, AUX, PUNCT]  0.000700   \n",
       "\n",
       "                                  Pretrained_Embeddings  \\\n",
       "1     [0.059051514, 0.21899414, 0.003074646, -0.0087...   \n",
       "2     [0.04988752, 0.14551653, -0.0022495815, 0.0167...   \n",
       "3     [0.0663147, 0.14313616, -0.0046561104, 0.01651...   \n",
       "4     [0.009531657, 0.08696832, 0.008748372, 0.01874...   \n",
       "5     [-0.04345703, -0.0020446777, 0.13183594, 0.025...   \n",
       "...                                                 ...   \n",
       "5890  [-0.0087890625, 0.16479492, -0.041625977, 0.01...   \n",
       "5891  [0.07126465, 0.19904785, -0.03173828, 0.031010...   \n",
       "5892  [0.034505207, 0.1398112, 0.07434082, 0.0005086...   \n",
       "5893  [0.21679688, 0.3828125, 0.024902344, -0.039306...   \n",
       "5894  [0.10986328, 0.2298177, 0.0029296875, 0.040974...   \n",
       "\n",
       "                                      Custom_Embeddings Sentiment_Score  \n",
       "1     [-0.021206133, 0.14562613, -0.037825022, -0.05...             0.0  \n",
       "2     [-0.01877182, 0.13091229, -0.03340487, -0.0473...             0.0  \n",
       "3     [-0.018213319, 0.13521256, -0.03366551, -0.048...             0.0  \n",
       "4     [-0.016320882, 0.12591653, -0.03068872, -0.045...             0.0  \n",
       "5     [-0.024421837, 0.16432925, -0.043763738, -0.06...             0.0  \n",
       "...                                                 ...             ...  \n",
       "5890  [-0.012502238, 0.1236767, -0.031620137, -0.048...             0.0  \n",
       "5891  [-0.01296855, 0.11175762, -0.027165066, -0.043...             0.0  \n",
       "5892  [-0.018213194, 0.09685853, -0.028583333, -0.03...             0.0  \n",
       "5893  [-0.020713434, 0.16700783, -0.042052798, -0.05...             0.0  \n",
       "5894  [-0.021759177, 0.10952186, -0.02987954, -0.040...             NaN  \n",
       "\n",
       "[5894 rows x 24 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "translations_list = []\n",
    "async def translate_bulk(sentences):\n",
    "    translator = Translator(service_urls=['translate.googleapis.com'])\n",
    "    translations = await translator.translate(sentences, dest='en')\n",
    "    for translation in translations:\n",
    "        translations_list.append(translation.text)\n",
    "    return translations_list\n",
    "\n",
    "translations_list = await translate_bulk(df['Sentence'].to_list())\n",
    "\n",
    "# Apply sentiment analysis on the translated sentences\n",
    "sentiment = pd.Series(translations_list).apply(lambda x: analyzer.polarity_scores(x))\n",
    "\n",
    "# Extract the compound score\n",
    "df['Sentiment_Score'] = sentiment.apply(lambda score_dict: score_dict['compound'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eca29fc935b639",
   "metadata": {},
   "source": [
    "### Noun Chunks in Sentences Using SpaCy\n",
    "\n",
    "It defines a function to count the number of noun chunks in each sentence and applies it to a DataFrame column:\n",
    "\n",
    "- The `count_noun_chunks` function:\n",
    "  - Processes the input text with SpaCy’s NLP pipeline.\n",
    "  - Extracts all noun chunks (contiguous noun phrases).\n",
    "  - Returns the count of these noun chunks.\n",
    "\n",
    "- The function is applied to the `\"Sentence\"` column of the DataFrame `df`, creating a new column `\"NounChunkCount\"` with the counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4308158870b47041",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T13:19:25.835704Z",
     "start_time": "2025-02-19T13:19:23.411383Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to count noun chunks\n",
    "def count_noun_chunks(text):\n",
    "    doc = nlp(text)\n",
    "    return len(list(doc.noun_chunks))\n",
    "\n",
    "# Apply to your DataFrame\n",
    "df['NounChunkCount'] = df['Sentence'].apply(count_noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e77cb9c376ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Exported to CSV:                                             Sentence   Emotion  POS_CCONJ  \\\n",
      "1  Bewust, niet bewust. ik vind het goed DAT het is.  surprise   0.000000   \n",
      "2     Er fysiek wel oordelen open, maar het is niet.   sadness   0.090909   \n",
      "3  Er staat fontein oordelen open, maar het is niet.   sadness   0.090909   \n",
      "4        Er staat bron voor open, maar het are niet.   sadness   0.090909   \n",
      "5                                         kram doen.   disgust   0.000000   \n",
      "\n",
      "   POS_VERB  POS_PRON  POS_SCONJ   POS_DET  POS_NOUN  POS_PUNCT   POS_ADJ  \\\n",
      "1  0.076923  0.230769   0.076923  0.000000  0.000000   0.230769  0.230769   \n",
      "2  0.090909  0.090909   0.000000  0.000000  0.000000   0.181818  0.181818   \n",
      "3  0.181818  0.090909   0.000000  0.000000  0.000000   0.181818  0.181818   \n",
      "4  0.090909  0.000000   0.000000  0.090909  0.090909   0.181818  0.181818   \n",
      "5  0.333333  0.000000   0.000000  0.000000  0.333333   0.333333  0.000000   \n",
      "\n",
      "   ...  POS_NUM  POS_SPACE  POS_SYM  POS_X  \\\n",
      "1  ...      0.0        0.0      0.0    0.0   \n",
      "2  ...      0.0        0.0      0.0    0.0   \n",
      "3  ...      0.0        0.0      0.0    0.0   \n",
      "4  ...      0.0        0.0      0.0    0.0   \n",
      "5  ...      0.0        0.0      0.0    0.0   \n",
      "\n",
      "                                            POS_tags    TF-IDF  \\\n",
      "1  [ADJ, PUNCT, ADV, ADJ, PUNCT, PRON, VERB, PRON...  0.000674   \n",
      "2  [ADV, ADJ, ADV, VERB, ADJ, PUNCT, CCONJ, PRON,...  0.000829   \n",
      "3  [ADV, VERB, ADJ, VERB, ADJ, PUNCT, CCONJ, PRON...  0.000845   \n",
      "4  [ADV, VERB, NOUN, ADP, ADJ, PUNCT, CCONJ, DET,...  0.000854   \n",
      "5                                [NOUN, VERB, PUNCT]  0.000423   \n",
      "\n",
      "                               Pretrained_Embeddings  \\\n",
      "1  [0.059051514, 0.21899414, 0.003074646, -0.0087...   \n",
      "2  [0.04988752, 0.14551653, -0.0022495815, 0.0167...   \n",
      "3  [0.0663147, 0.14313616, -0.0046561104, 0.01651...   \n",
      "4  [0.009531657, 0.08696832, 0.008748372, 0.01874...   \n",
      "5  [-0.04345703, -0.0020446777, 0.13183594, 0.025...   \n",
      "\n",
      "                                   Custom_Embeddings  Sentiment_Score  \\\n",
      "1  [-0.021206133, 0.14562613, -0.037825022, -0.05...              0.0   \n",
      "2  [-0.01877182, 0.13091229, -0.03340487, -0.0473...              0.0   \n",
      "3  [-0.018213319, 0.13521256, -0.03366551, -0.048...              0.0   \n",
      "4  [-0.016320882, 0.12591653, -0.03068872, -0.045...              0.0   \n",
      "5  [-0.024421837, 0.16432925, -0.043763738, -0.06...              0.0   \n",
      "\n",
      "  NounChunkCount  \n",
      "1              2  \n",
      "2              1  \n",
      "3              1  \n",
      "4              1  \n",
      "5              1  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.to_csv('NLP_features_Test.csv', index=False)\n",
    "print(\"DataFrame Exported to CSV:\", df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blockC-y2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
