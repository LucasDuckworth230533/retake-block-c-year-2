{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721e0321-69fa-42b3-a952-ba8a76881aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 00:00:08.360283: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-27 00:00:08.372651: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750982408.387080 1005914 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750982408.391478 1005914 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750982408.403102 1005914 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750982408.403120 1005914 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750982408.403123 1005914 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750982408.403125 1005914 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-27 00:00:08.407038: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24257b6e-281a-489f-b079-ca47f7e6fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6039f083-12e3-4a1d-826a-3250d6e5d46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "training = pd.read_csv(\"train_dataset.csv\")\n",
    "test = pd.read_csv(\"test_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "953f4fe8-684a-4391-8157-4814e7406ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = training[\"Sentence\"].astype(str)\n",
    "y = training[\"Emotion\"]\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c695332-2549-4c01-a9ec-0557277900f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "# Load test data\n",
    "X_test = test['Sentence'].astype(str)\n",
    "y_test = test['Emotion']\n",
    "\n",
    "y_test_encoded = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6befc03-c865-4db2-b2ac-698d144fce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERTje tokenizer\n",
    "model_name = \"wietsedv/bert-base-dutch-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bd3db1c-2797-45cb-bfc8-106dfc7c9645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "040822e7-19a6-490e-8053-b741ecb0d93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = EmotionDataset(X_train.tolist(), y_train, tokenizer)\n",
    "val_dataset = EmotionDataset(X_val.tolist(), y_val, tokenizer)\n",
    "test_dataset = EmotionDataset(X_test.tolist(), y_test_encoded, tokenizer)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78423487-8606-44c9-a64a-ee862e6d4af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at wietsedv/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 1.3364, Train Acc = 0.5032\n",
      "Validation Acc = 0.6964, Validation F1 = 0.6959\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       1.00      0.71      0.83        49\n",
      "     disgust       0.88      0.68      0.77        75\n",
      "        fear       0.67      0.93      0.78       198\n",
      "   happiness       0.52      0.73      0.61       190\n",
      "     neutral       0.72      0.53      0.61       242\n",
      "     sadness       0.79      0.68      0.73       209\n",
      "    surprise       0.75      0.65      0.70       216\n",
      "\n",
      "    accuracy                           0.70      1179\n",
      "   macro avg       0.76      0.70      0.72      1179\n",
      "weighted avg       0.72      0.70      0.70      1179\n",
      "\n",
      "Epoch 2: Train Loss = 0.5301, Train Acc = 0.8242\n",
      "Validation Acc = 0.8134, Validation F1 = 0.8120\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       1.00      0.96      0.98        49\n",
      "     disgust       0.95      0.93      0.94        75\n",
      "        fear       0.89      0.95      0.92       198\n",
      "   happiness       0.66      0.65      0.66       190\n",
      "     neutral       0.70      0.67      0.69       242\n",
      "     sadness       0.90      0.87      0.88       209\n",
      "    surprise       0.82      0.87      0.84       216\n",
      "\n",
      "    accuracy                           0.81      1179\n",
      "   macro avg       0.85      0.84      0.84      1179\n",
      "weighted avg       0.81      0.81      0.81      1179\n",
      "\n",
      "Epoch 3: Train Loss = 0.2200, Train Acc = 0.9326\n",
      "Validation Acc = 0.8134, Validation F1 = 0.8084\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       1.00      0.92      0.96        49\n",
      "     disgust       0.95      0.97      0.96        75\n",
      "        fear       0.84      0.98      0.91       198\n",
      "   happiness       0.65      0.66      0.66       190\n",
      "     neutral       0.73      0.58      0.65       242\n",
      "     sadness       0.86      0.93      0.89       209\n",
      "    surprise       0.86      0.86      0.86       216\n",
      "\n",
      "    accuracy                           0.81      1179\n",
      "   macro avg       0.84      0.84      0.84      1179\n",
      "weighted avg       0.81      0.81      0.81      1179\n",
      "\n",
      "Final Test Acc = 0.4454, Test F1 = 0.4406\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.60      0.07      0.12        46\n",
      "     disgust       0.00      0.00      0.00         2\n",
      "        fear       0.12      0.47      0.20        17\n",
      "   happiness       0.61      0.72      0.66       243\n",
      "     neutral       0.54      0.39      0.45       238\n",
      "     sadness       0.15      0.57      0.24        30\n",
      "    surprise       0.34      0.18      0.23       147\n",
      "\n",
      "    accuracy                           0.45       723\n",
      "   macro avg       0.34      0.34      0.27       723\n",
      "weighted avg       0.50      0.45      0.44       723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load BERTje model\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))\n",
    "model.to(device)\n",
    "\n",
    "# Training setup\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, train_loader):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    predictions, true_labels = [], []\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['label'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "    return total_loss / len(train_loader), accuracy_score(true_labels, predictions)\n",
    "\n",
    "def evaluate(model, train_loader):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in train_loader:\n",
    "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['label'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    acc = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Convert numerical predictions back to emotion names for the classification report\n",
    "    pred_emotions = label_encoder.inverse_transform(predictions)\n",
    "    true_emotions = label_encoder.inverse_transform(true_labels)\n",
    "    \n",
    "    return acc, f1, classification_report(true_emotions, pred_emotions)\n",
    "\n",
    "for epoch in range(3):\n",
    "    train_loss, train_acc = train(model, train_loader)  # Train on train_loader\n",
    "    val_acc, val_f1, val_report = evaluate(model, val_loader)  # Evaluate on validation set\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Train Acc = {train_acc:.4f}\")\n",
    "    print(f\"Validation Acc = {val_acc:.4f}, Validation F1 = {val_f1:.4f}\")\n",
    "    print(val_report)\n",
    "\n",
    "test_acc, test_f1, test_report = evaluate(model, test_loader)\n",
    "print(f\"Final Test Acc = {test_acc:.4f}, Test F1 = {test_f1:.4f}\")\n",
    "print(test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e7c97-7404-48e4-921b-476382926f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "# Get predictions on test set\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        \n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "# Convert numerical labels to emotion names\n",
    "y_true_emotions = label_encoder.inverse_transform(y_true)\n",
    "y_pred_emotions = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "# Get unique emotions that appear in predictions\n",
    "unique_emotions = np.unique(np.concatenate([y_true_emotions, y_pred_emotions]))\n",
    "\n",
    "# Compute confusion matrix with emotion names\n",
    "cm = confusion_matrix(y_true_emotions, y_pred_emotions, labels=pred_emotions)\n",
    "\n",
    "# Convert numerical labels to categorical labels using label_encoder\n",
    "filtered_labels = label_encoder.inverse_transform(unique_labels)  # Convert numbers to category names\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=filtered_labels, yticklabels=filtered_labels)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Print emotion mapping for reference\n",
    "print(\"\\nEmotion Label Mapping:\")\n",
    "for i, emotion in enumerate(label_encoder.classes_):\n",
    "    print(f\"{i}: {emotion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d751ce2-bc8c-4532-b78a-70c60a60b1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: ./finetuned-bertje_emotion_classifier\n",
      "Files saved:\n",
      "- pytorch_model.bin (model weights)\n",
      "- config.json (model configuration)\n",
      "- tokenizer.json (tokenizer)\n",
      "- vocab.txt (vocabulary)\n",
      "- label_encoder.pkl (label encoder)\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "model_save_path = \"./finetuned-bertje_emotion_classifier\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "# Also save the label encoder for future use\n",
    "import joblib\n",
    "joblib.dump(label_encoder, os.path.join(model_save_path, \"label_encoder.pkl\"))\n",
    "\n",
    "print(f\"Model saved to: {model_save_path}\")\n",
    "print(\"Files saved:\")\n",
    "print(\"- pytorch_model.bin (model weights)\")\n",
    "print(\"- config.json (model configuration)\")\n",
    "print(\"- tokenizer.json (tokenizer)\")\n",
    "print(\"- vocab.txt (vocabulary)\")\n",
    "print(\"- label_encoder.pkl (label encoder)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ded9aa8-cecb-4b80-8e90-3291c8b2b88d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
