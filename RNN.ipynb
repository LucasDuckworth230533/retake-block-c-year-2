{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aaec1211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SpatialDropout1D, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "025f199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../task4/NLP_features 2.csv')\n",
    "train_df = pd.read_csv('../datasets/Train_Test_data/Training_dataset.csv')\n",
    "# Parameters\n",
    "MAX_NUM_WORDS = 5000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7887f4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion\n",
      "neutral      1209\n",
      "surprise     1082\n",
      "sadness      1047\n",
      "fear          988\n",
      "happiness     950\n",
      "disgust       376\n",
      "anger         243\n",
      "Name: count, dtype: int64\n",
      "Emotion\n",
      "neutral      255\n",
      "happiness    250\n",
      "surprise     147\n",
      "anger         47\n",
      "sadness       31\n",
      "fear          17\n",
      "disgust        2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print count of each class in the training set\n",
    "print(train_df['Emotion'].value_counts())\n",
    "# print count of each class in the test set\n",
    "print(test_df['Emotion'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44253ee9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7a83b6",
   "metadata": {},
   "source": [
    "# Create first RNN model attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "150de753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beheerder\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - accuracy: 0.2384 - loss: 1.8739 - val_accuracy: 0.2273 - val_loss: 1.8513\n",
      "Epoch 2/5\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 51ms/step - accuracy: 0.4476 - loss: 1.5171 - val_accuracy: 0.3376 - val_loss: 1.5560\n",
      "Epoch 3/5\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 52ms/step - accuracy: 0.6935 - loss: 0.9031 - val_accuracy: 0.5208 - val_loss: 1.1667\n",
      "Epoch 4/5\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 51ms/step - accuracy: 0.8322 - loss: 0.5268 - val_accuracy: 0.5530 - val_loss: 1.1645\n",
      "Epoch 5/5\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.8955 - loss: 0.3569 - val_accuracy: 0.5403 - val_loss: 1.2301\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3441 - loss: 2.3475\n",
      "\n",
      "Test Accuracy: 0.33\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.00      0.00      0.00        47\n",
      "     disgust       0.00      0.00      0.00         2\n",
      "        fear       0.09      0.29      0.14        17\n",
      "   happiness       0.60      0.30      0.40       250\n",
      "     neutral       0.41      0.51      0.46       255\n",
      "     sadness       0.08      0.35      0.13        31\n",
      "    surprise       0.33      0.20      0.25       147\n",
      "\n",
      "    accuracy                           0.33       749\n",
      "   macro avg       0.21      0.24      0.19       749\n",
      "weighted avg       0.41      0.33      0.34       749\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['Encoded_Emotion'] = label_encoder.fit_transform(train_df['Emotion'])\n",
    "test_df['Encoded_Emotion'] = label_encoder.transform(test_df['Emotion'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Tokenizer (fit only on training data)\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_df['Sentence'])\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess_text(texts):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    return padded\n",
    "\n",
    "X_train = preprocess_text(train_df['Sentence'])\n",
    "X_test = preprocess_text(test_df['Sentence'])\n",
    "\n",
    "y_train = train_df['Encoded_Emotion'].values\n",
    "y_test = test_df['Encoded_Emotion'].values\n",
    "\n",
    "# Build the model\n",
    "def create_rnn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=MAX_NUM_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model = create_rnn_model()\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate on test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Print detailed classification report\n",
    "class_names = label_encoder.classes_\n",
    "report = classification_report(y_test, y_pred, target_names=class_names)\n",
    "print(\"\\nClassification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f920c471",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d571c90",
   "metadata": {},
   "source": [
    "# Second Iteration with Bidirectional LSTM with Dutch Lemmatization, FastText Embeddings, and Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d79bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.2434 - loss: 1.9872\n",
      "\n",
      "Test Accuracy: 0.22\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.09      0.04      0.06        47\n",
      "     disgust       0.00      0.00      0.00         2\n",
      "        fear       0.03      0.12      0.04        17\n",
      "   happiness       0.67      0.16      0.25       250\n",
      "     neutral       0.46      0.31      0.37       255\n",
      "     sadness       0.05      0.29      0.09        31\n",
      "    surprise       0.24      0.26      0.25       147\n",
      "\n",
      "    accuracy                           0.22       749\n",
      "   macro avg       0.22      0.17      0.15       749\n",
      "weighted avg       0.44      0.22      0.27       749\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy Dutch model\n",
    "nlp = spacy.load('nl_core_news_sm')\n",
    "\n",
    "# Parameters\n",
    "MAX_NUM_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 300\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Lemmatization function for Dutch using spaCy\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "# Apply lemmatization\n",
    "train_df['Lemmatized_Sentence'] = train_df['Sentence'].apply(lemmatize_text)\n",
    "test_df['Lemmatized_Sentence'] = test_df['Sentence'].apply(lemmatize_text)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['Encoded_Emotion'] = label_encoder.fit_transform(train_df['Emotion'])\n",
    "test_df['Encoded_Emotion'] = label_encoder.transform(test_df['Emotion'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_df['Lemmatized_Sentence'])\n",
    "\n",
    "def preprocess_text(texts):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "X_train = preprocess_text(train_df['Lemmatized_Sentence'])\n",
    "X_test = preprocess_text(test_df['Lemmatized_Sentence'])\n",
    "\n",
    "y_train = train_df['Encoded_Emotion'].values\n",
    "y_test = test_df['Encoded_Emotion'].values\n",
    "\n",
    "# Load FastText Dutch embeddings\n",
    "embedding_index = {}\n",
    "embedding_path = 'cc.nl.300.vec'\n",
    "\n",
    "with open(embedding_path, encoding='utf8') as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        values = line.rstrip().split(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "# Create embedding matrix\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((MAX_NUM_WORDS, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Build the model\n",
    "def create_bidirectional_lstm():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=MAX_NUM_WORDS,\n",
    "                        output_dim=EMBEDDING_DIM,\n",
    "                        input_length=MAX_SEQUENCE_LENGTH,\n",
    "                        weights=[embedding_matrix],\n",
    "                        trainable=True))\n",
    "    model.add(SpatialDropout1D(0.3))\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)))\n",
    "    model.add(Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.3)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer=Adam(learning_rate=0.001),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_train, y_train,\n",
    "            epochs=50,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            validation_split=0.2,\n",
    "            class_weight=class_weight_dict,\n",
    "            callbacks=[early_stop])\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
    "print(\"\\nClassification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc3ced5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b65d0e5",
   "metadata": {},
   "source": [
    "# Third iteration on RNN model with Data Augmentation via Random Swap & FastText Embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bf702cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented test data distribution:\n",
      " Emotion\n",
      "neutral      255\n",
      "happiness    250\n",
      "fear         150\n",
      "sadness      150\n",
      "anger        150\n",
      "disgust      150\n",
      "surprise     147\n",
      "Name: count, dtype: int64\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Beheerder\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 1s/step - accuracy: 0.1501 - loss: 1.9912 - val_accuracy: 0.0280 - val_loss: 2.1064\n",
      "Epoch 2/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 2s/step - accuracy: 0.2902 - loss: 1.6483 - val_accuracy: 0.1433 - val_loss: 1.7862\n",
      "Epoch 3/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 1s/step - accuracy: 0.5783 - loss: 1.0121 - val_accuracy: 0.3562 - val_loss: 1.4845\n",
      "Epoch 4/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 2s/step - accuracy: 0.7177 - loss: 0.6803 - val_accuracy: 0.3978 - val_loss: 1.3365\n",
      "Epoch 5/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 1s/step - accuracy: 0.7813 - loss: 0.4939 - val_accuracy: 0.4224 - val_loss: 1.4792\n",
      "Epoch 6/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 2s/step - accuracy: 0.8057 - loss: 0.4419 - val_accuracy: 0.4249 - val_loss: 1.5159\n",
      "Epoch 7/10\n",
      "\u001b[1m74/74\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 1s/step - accuracy: 0.8433 - loss: 0.3741 - val_accuracy: 0.4936 - val_loss: 1.3825\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 200ms/step - accuracy: 0.2862 - loss: 2.2517\n",
      "\n",
      "Test Accuracy: 0.28\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 209ms/step\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.14      0.02      0.04       150\n",
      "     disgust       0.48      0.50      0.49       150\n",
      "        fear       0.42      0.16      0.23       150\n",
      "   happiness       0.31      0.30      0.30       250\n",
      "     neutral       0.28      0.35      0.31       255\n",
      "     sadness       0.26      0.31      0.28       150\n",
      "    surprise       0.15      0.29      0.20       147\n",
      "\n",
      "    accuracy                           0.28      1252\n",
      "   macro avg       0.29      0.27      0.26      1252\n",
      "weighted avg       0.29      0.28      0.27      1252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Augmentation for underrepresented classes\n",
    "underrepresented = ['disgust', 'fear', 'anger', 'sadness']\n",
    "min_samples = 150\n",
    "\n",
    "def random_swap(text, n_swaps=1):\n",
    "    words = text.split()\n",
    "    for _ in range(n_swaps):\n",
    "        if len(words) < 2:\n",
    "            break\n",
    "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def augment_class_swap(df, emotion_label, target_count):\n",
    "    subset = df[df['Emotion'] == emotion_label]\n",
    "    augmented_texts = []\n",
    "    current_count = len(subset)\n",
    "    \n",
    "    while current_count < target_count:\n",
    "        for text in subset['Sentence']:\n",
    "            augmented_text = random_swap(text)\n",
    "            augmented_texts.append(augmented_text)\n",
    "            current_count += 1\n",
    "            if current_count >= target_count:\n",
    "                break\n",
    "    \n",
    "    new_df = pd.DataFrame({'Sentence': augmented_texts, 'Emotion': [emotion_label]*len(augmented_texts)})\n",
    "    return pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "for emotion in underrepresented:\n",
    "    count = test_df[test_df['Emotion'] == emotion].shape[0]\n",
    "    if count < min_samples:\n",
    "        test_df = augment_class_swap(test_df, emotion, min_samples)\n",
    "\n",
    "print(\"Augmented test data distribution:\\n\", test_df['Emotion'].value_counts())\n",
    "\n",
    "# Lemmatization using spaCy\n",
    "nlp = spacy.load('nl_core_news_sm')\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "train_df['Lemmatized_Sentence'] = train_df['Sentence'].apply(lemmatize_text)\n",
    "test_df['Lemmatized_Sentence'] = test_df['Sentence'].apply(lemmatize_text)\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['Encoded_Emotion'] = label_encoder.fit_transform(train_df['Emotion'])\n",
    "test_df['Encoded_Emotion'] = label_encoder.transform(test_df['Emotion'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Constants\n",
    "MAX_NUM_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 300\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_df['Lemmatized_Sentence'])\n",
    "\n",
    "def preprocess_text(texts):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "X_train = preprocess_text(train_df['Lemmatized_Sentence'])\n",
    "X_test = preprocess_text(test_df['Lemmatized_Sentence'])\n",
    "\n",
    "y_train = train_df['Encoded_Emotion'].values\n",
    "y_test = test_df['Encoded_Emotion'].values\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Build model without pre-trained embeddings\n",
    "def create_bidirectional_lstm():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=MAX_NUM_WORDS,\n",
    "                        output_dim=EMBEDDING_DIM,\n",
    "                        input_length=MAX_SEQUENCE_LENGTH,\n",
    "                        trainable=True))  # <-- learned from scratch\n",
    "    model.add(SpatialDropout1D(0.3))\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)))\n",
    "    model.add(Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.3)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train model\n",
    "model = create_bidirectional_lstm()\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          epochs=EPOCHS,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          validation_split=0.2,\n",
    "          class_weight=class_weight_dict,\n",
    "          callbacks=[early_stop])\n",
    "\n",
    "# Evaluate\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Predictions and report\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
    "print(\"\\nClassification Report:\\n\", report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
